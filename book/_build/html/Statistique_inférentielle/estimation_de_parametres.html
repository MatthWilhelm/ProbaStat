
<!DOCTYPE html>

<html lang="fr">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>2. Estimation de paramètres &#8212; Probabilité et statistique (MATH-234(d))</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://MatthWilhelm.github.io/ProbaStat/Statistique_inférentielle/estimation_de_parametres.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Recherche" href="../search.html" />
    <link rel="next" title="3. Propriétés d’un estimateur" href="proprietes_estimateur.html" />
    <link rel="prev" title="1. Introduction" href="introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="fr">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/EPFL_Logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Probabilité et statistique (MATH-234(d))</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Probabilités et Statistique
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  À propos du cours
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Introduction.html">
   Introduction
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Statistique Exploratoire
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistiques_exploratoire/types_de_donn%C3%A9es.html">
   1. Types de données
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistiques_exploratoire/graphiques.html">
   2. Graphiques
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistiques_exploratoire/synth%C3%A8ses_num%C3%A9riques.html">
   3. Synthèses numériques
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistiques_exploratoire/boxplot.html">
   4. Boxplot
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistiques_exploratoire/loi_normale.html">
   5. Loi normale
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Statistiques_exploratoire/quelques_principes_g%C3%A9n%C3%A9raux.html">
   6. Quelques principes généraux
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Probabilité
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Probabilit%C3%A9/concepts_de_base.html">
   1. Concepts de base
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Probabilit%C3%A9/arrangements_et_combinaisons.html">
   2. Arrangements et combinaisons
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Probabilit%C3%A9/probabilite_conditionnelle_independance.html">
   3. Probabilité conditionelle et indépendence
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Probabilit%C3%A9/probabilites_totales_et_theoreme_de_bayes.html">
   4. Probabilités totales et théorème de Bayes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Probabilit%C3%A9/variables_aleatoires_discretes.html">
   5. Variables aléatoires discrètes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Probabilit%C3%A9/variables_aleatoires_continues.html">
   6. Variables aléatoires continues
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Probabilit%C3%A9/variables_aleatoires_conjointes.html">
   7. Variables aléatoires conjointes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Probabilit%C3%A9/valeurs_caracteristiques.html">
   8. Valeurs caractéristiques
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Probabilit%C3%A9/theoreme_fondamentaux.html">
   9. Théorème fondamentaux
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Probabilit%C3%A9/test_notebook.html">
   10. Test live notebook
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Statistique inférentielle
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. Estimation de paramètres
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="proprietes_estimateur.html">
   3. Propriétés d’un estimateur
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="estimation_par_intervalle.html">
   4. Estimation par intervalle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tests_hypotheses_statistiques.html">
   5. Tests d’hypothèses statistiques
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tests_et_ic.html">
   6. Tests et IC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="test_chi2.html">
   7. Test du
   <span class="math notranslate nohighlight">
    \(\chi^2\)
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="puissance_test.html">
   8. Puissance d’un test
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Régréssion linéaire
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../R%C3%A9gr%C3%A9ssion_lin%C3%A9aire/introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../R%C3%A9gr%C3%A9ssion_lin%C3%A9aire/cas_general.html">
   2. Régréssion linéaire: cas général
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../R%C3%A9gr%C3%A9ssion_lin%C3%A9aire/tests.html">
   3. Régréssion linéaire: tests
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../R%C3%A9gr%C3%A9ssion_lin%C3%A9aire/hypotheses_et_diagnostics.html">
   4. Régréssion linéaire: hypothèses et diagnostics
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/MatthWilhelm/ProbaStat"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/MatthWilhelm/ProbaStat/issues/new?title=Issue%20on%20page%20%2FStatistique_inférentielle/estimation_de_parametres.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/MatthWilhelm/ProbaStat/edit/master/book/Statistique_inférentielle/estimation_de_parametres.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/Statistique_inférentielle/estimation_de_parametres.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#methode-des-moments">
   2.1. Méthode des moments
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#methode-du-maximum-de-vraisemblance">
   2.2. Méthode du maximum de vraisemblance
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Estimation de paramètres</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#methode-des-moments">
   2.1. Méthode des moments
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#methode-du-maximum-de-vraisemblance">
   2.2. Méthode du maximum de vraisemblance
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="estimation-de-parametres">
<h1><span class="section-number">2. </span>Estimation de paramètres<a class="headerlink" href="#estimation-de-parametres" title="Lien permanent vers ce titre">#</a></h1>
<div class="math notranslate nohighlight">
\[\newcommand\var{\text{Var}}
\newcommand\E{\mathbb{E}}\]</div>
<p>Pour estimer les paramètres d’un modèles à partir des données, plusieurs méthodes peuvent être utilisées:</p>
<ul class="simple">
<li><p>méthode des moments;</p></li>
<li><p>méthode du maximum de vraisemblance.</p></li>
</ul>
<div class="proof definition admonition" id="estmiateur">
<p class="admonition-title"><span class="caption-number">Définition 2.3 </span> (Estimateur)</p>
<section class="definition-content" id="proof-content">
<p>On commencé par faire l’hypothèse d’un modèle <span class="math notranslate nohighlight">\(F_\theta\)</span>, complètement caractérisé par un paramètre (éventuellement un vecteur) <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Cela signifie que pour connaître notre modèle, il suffit de connaître <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Un estimateur <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>  de notre paramètre <span class="math notranslate nohighlight">\(\theta\)</span> est une valeur dépendant de <span class="math notranslate nohighlight">\(X_1, \dots, X_n\)</span> telle qu’elle estime la valeur de <span class="math notranslate nohighlight">\(\theta\)</span> qui a engendré <span class="math notranslate nohighlight">\(X_1, \dots, X_n\)</span>.</p>
<p>Comme <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> dépend des variables aléatoires <span class="math notranslate nohighlight">\(X_1, \dots, X_n\)</span>, c’est bien une variable aléatoire.</p>
</section>
</div><div class="admonition-notations admonition">
<p class="admonition-title">Notations</p>
<p>L’estimateur d’un paramètre donné est généralement noté avec le même symbole surmonté d’un chapeau. Par exemple, si on veut estimer <span class="math notranslate nohighlight">\(\theta\)</span>, on utilisera la notation  <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> pour en désigner un estimateur. Le « chapeau » signifie grossièrement que c’est une statistique (une variable aléatoire donc) dont le but est d’estimer le paramètre <span class="math notranslate nohighlight">\(\theta\)</span>. Il s’agit d’une notation: cela ne dit rien quant aux propriétés de l’estimateur en question.</p>
</div>
<section id="methode-des-moments">
<h2><span class="section-number">2.1. </span>Méthode des moments<a class="headerlink" href="#methode-des-moments" title="Lien permanent vers ce titre">#</a></h2>
<div class="proof definition admonition" id="meth_moment">
<p class="admonition-title"><span class="caption-number">Définition 2.4 </span> (Méthode des moments)</p>
<section class="definition-content" id="proof-content">
<p>Probablement la plus intuitive et la plus ancienne des méthodes d’estimation. Étant donné un modèle <span class="math notranslate nohighlight">\(F_\theta\)</span>, on peut calculer <em>les moments</em> de la distribution <span class="math notranslate nohighlight">\(F_\theta\)</span>, donnés par</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*} m_k := \E[X^k]. \end{gather*}\]</div>
<p>Supposons que l’on ait <span class="math notranslate nohighlight">\(X_1, \dots,X_n \stackrel{idd}{\sim} F_\theta \)</span>. La loi des grand nombres justifie asymptotiquement que l’on utilise</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*} \hat{m}_k = \frac{1}{n} \sum_{i = 1}^n X_i^k,\end{gather*}\]</div>
<p>comme estimateur de <span class="math notranslate nohighlight">\(m_k\)</span>. Finalement, puisque <span class="math notranslate nohighlight">\(m_k\)</span> dépend directement de <span class="math notranslate nohighlight">\(\theta\)</span>, alors on estime <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> tel que <span class="math notranslate nohighlight">\(\hat{m}_k  = m_k\)</span>.</p>
</section>
</div><div class="proof example admonition" id="ex1_moment">
<p class="admonition-title"><span class="caption-number">Exemple 2.4 </span> (Méthode des moments)</p>
<section class="example-content" id="proof-content">
<p>Supposons que l’on observe <span class="math notranslate nohighlight">\(x_1,\dots,x_n\)</span>, que l’on modélise comme des réalisations d’une loi normale, de paramètres <span class="math notranslate nohighlight">\(\mu\)</span> et <span class="math notranslate nohighlight">\(\sigma^2\)</span> respectivement. On pose donc <span class="math notranslate nohighlight">\(X_1, \dots, X_n \stackrel{idd}{\sim} \mathcal{N}(\mu, \sigma^2)\)</span>.</p>
<p>En utilisant la méthode des moments, estimer les paramètres <span class="math notranslate nohighlight">\(\mu\)</span> et <span class="math notranslate nohighlight">\(\sigma^2\)</span> caractérisant la distribution normale. On rappelle que pour une variable <span class="math notranslate nohighlight">\(X\sim\mathcal{N}(\mu, \sigma^2),\)</span> on a</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*}\E[X] = \mu,\quad \var(X) = \sigma^2 \Leftrightarrow  \E[X^2] = \sigma^2 + \mu^2.  \end{gather*}\]</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution</p>
<p>On pose</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*}
\hat{m}_1 =\frac{1}{n} \sum_{i = 1}^n X_i = \overline{X},\quad \hat{m}_2 = \frac{1}{n} \sum_{i = 1}^n X^2_i. 
\end{gather*}\]</div>
<p>Les conditions <span class="math notranslate nohighlight">\(\hat{m}_1 = m_1\)</span> et <span class="math notranslate nohighlight">\(\hat{m}_2= m_2\)</span> donnent lieu au système suivant:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*}
\left\{
\begin{array}{ll}
\hat{m}_1 &amp;=\hat{\mu}  \\
\hat{m}_2 &amp;= \hat{\sigma}^2 + \hat{\mu}^2
\end{array}
\right.
\Leftrightarrow
\left\{
\renewcommand{\arraystretch}{1.5}
\begin{array}{ll}
\hat{\mu} &amp;=\hat{m}_1 \\
\hat{\sigma}^2 &amp;= \hat{m}_2 -\hat{m}_1^2.
\end{array}
\right.
\end{gather*}\]</div>
</div>
</section>
</div><div class="proof example admonition" id="ex2_moment">
<p class="admonition-title"><span class="caption-number">Exemple 2.5 </span> (Méthode des moments)</p>
<section class="example-content" id="proof-content">
<p>Supposons que l’on observe <span class="math notranslate nohighlight">\(x_1,\dots,x_n\)</span>, que l’on modélise comme des réalisations d’une loi Gamma, de paramètres de forme et d’échelle <span class="math notranslate nohighlight">\(k\)</span> et <span class="math notranslate nohighlight">\(\theta\)</span> respectivement. On pose donc <span class="math notranslate nohighlight">\(X_1, \dots, X_n \stackrel{idd}{\sim} \Gamma(k, \theta)\)</span>.
En utilisant la méthode des moments, estimer les paramètres <span class="math notranslate nohighlight">\(k\)</span> et <span class="math notranslate nohighlight">\(\theta\)</span> caractérisant la distribution Gamma. On rappelle que pour une variable <span class="math notranslate nohighlight">\(X\sim\Gamma(k,\theta),\)</span> on a</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*}\E[X] = k\theta,\end{gather*}\]</div>
<p>ainsi que</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*} \var(X) = k\theta^2 \Leftrightarrow  \E[X^2] = k\theta^2 + k^2\theta^2= \theta^2 k(k +1 ) .  \end{gather*}\]</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution</p>
<p>On pose</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*}
\hat{m}_1 =\frac{1}{n} \sum_{i = 1}^n X_i = \overline{X},\quad \hat{m}_2 = \frac{1}{n} \sum_{i = 1}^n X^2_i. \end{gather*}\]</div>
<p>Les conditions <span class="math notranslate nohighlight">\(\hat{m}_1 = m_1\)</span> et <span class="math notranslate nohighlight">\(\hat{m}_2= m_2\)</span> donnent lieu au système suivant:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*}
\left\{
\begin{array}{ll}
\hat{m}_1 &amp;= \hat{k}\hat{\theta}, \\
\hat{m}_2 &amp;= \hat{\theta}^2 \hat{k}(\hat{k} +1 ),
\end{array}
\right.
\Leftrightarrow
\left\{
\renewcommand{\arraystretch}{2}
\begin{array}{ll}
\hat{\theta} &amp;=\dfrac{\hat{m}_2 -\hat{m}_1^2}{\hat{m}_1}, \\
\hat{k} &amp;= \dfrac{\hat{m}_1^2 }{\hat{m}_2 -\hat{m}_1^2 }.
\end{array}
\right.
\end{gather*}\]</div>
</div>
</section>
</div></section>
<section id="methode-du-maximum-de-vraisemblance">
<h2><span class="section-number">2.2. </span>Méthode du maximum de vraisemblance<a class="headerlink" href="#methode-du-maximum-de-vraisemblance" title="Lien permanent vers ce titre">#</a></h2>
<p>Cette méthode est <strong>extrêmement générale</strong>.</p>
<div class="proof definition admonition" id="vraisemblance">
<p class="admonition-title"><span class="caption-number">Définition 2.5 </span> (Vraisemblance)</p>
<section class="definition-content" id="proof-content">
<p>Soient <span class="math notranslate nohighlight">\( X_1 \dots X_n \stackrel{idd}{\sim} f(x;\theta)\)</span>. La <strong>vraisemblance</strong> est la fonction de <span class="math notranslate nohighlight">\( \theta \)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*} L(\theta) = \prod_{i=1}^n f(X_i; \theta).\end{gather*}\]</div>
<p>Formellement, la fonction de vraisemblance est une <strong>fonction aléatoire</strong>, elle dépend de l’échantillon aléatoire <span class="math notranslate nohighlight">\(X_1 \dots X_n\)</span>.</p>
</section>
</div><p>Si on fait l’hypothèse que <span class="math notranslate nohighlight">\( x_1 \dots x_n\)</span> est un échantillon observé que l’on modélise par <span class="math notranslate nohighlight">\( X_1 \dots X_n \stackrel{idd}{\sim} f(x;\theta) \)</span>, alors <span class="math notranslate nohighlight">\(L(\theta) \)</span> est simplement l’évaluation de la densité jointe au point <span class="math notranslate nohighlight">\((x_1,\dots,x_n)\)</span>, c’est-à-dire</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*}L(\theta) = f_{X_1,\dots,X_n}(x_1,\dots,x_n; \theta). \end{gather*}\]</div>
<p>Autrement dit, de manière informelle, elle représente la « probabilité de l’événement »</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*}\mathbb{P}(X_1 = x_1, \dots, X_n = x_n),\end{gather*}\]</div>
<p>comme fonction de <span class="math notranslate nohighlight">\(\theta\)</span>.<br />
(Attention, puisqu’il s’agit de densités, cette probabilité est nulle.)</p>
<p>Si on considère <strong>un modèle</strong>, c’est-à-dire que l’on s’intéresse à la vraisemblance et non à une <strong>réalisation</strong>, alors on écrit</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*}L(\theta) = f_{X_1,\dots,X_n}(X_1,\dots,X_n; \theta), \end{gather*}\]</div>
<p>et il s’agit d’une <strong>fonction aléatoire</strong>, puisqu’elle dépend des variables aléatoires <span class="math notranslate nohighlight">\(X_1, \dots, X_n\)</span>.</p>
<div class="proof definition admonition" id="est_max_vraisemblance">
<p class="admonition-title"><span class="caption-number">Définition 2.6 </span> (Estimateur du maximum de vraisemblance)</p>
<section class="definition-content" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\(L:\Theta \rightarrow \mathbb{R}\)</span> une fonction de vraisemblance. Alors
<strong>l’estimateur du maximum de vraisemblance</strong>, noté <span class="math notranslate nohighlight">\(\hat{\theta}_{ML} \)</span> satisfait</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*} L\left(\hat{\theta}_{ML}\right) \geq L(\theta), \quad \forall \theta\in \Theta. \end{gather*}\]</div>
<p>Autrement dit, <span class="math notranslate nohighlight">\(\hat{\theta}_{ML}\)</span> représente le paramètre le plus vraisemblable étant donné une famille de distributions <span class="math notranslate nohighlight">\(f_\theta\)</span> et des données <span class="math notranslate nohighlight">\(X_1,\dots, X_n.\)</span></p>
</section>
</div><p>Il est plus facile de maximiser <span class="math notranslate nohighlight">\( \ell(\theta) = \log[ L(\theta) ] \)</span>. On appelle la fonction <span class="math notranslate nohighlight">\(\ell\)</span> la fonction de <strong>log-vraisemblance</strong>. On procède en général de la manière suivante:</p>
<ul class="simple">
<li><p>On calcule <span class="math notranslate nohighlight">\(\ell\)</span>, la fonction de  log-vraisemblance du modèle.</p></li>
<li><p>On calcule la dérivée: <span class="math notranslate nohighlight">\( \ell'(\theta) \)</span>.</p></li>
<li><p>On résout l’équation <span class="math notranslate nohighlight">\( \ell'(\theta) = 0 \)</span> (peut être fait numériquement).</p></li>
<li><p>On vérifie qu’il s’agit bien d’un maximum.</p></li>
</ul>
<div class="proof example admonition" id="example-6">
<p class="admonition-title"><span class="caption-number">Exemple 2.6 </span> (Estimation par maximum de vraisemblance)</p>
<section class="example-content" id="proof-content">
<p>Supposons que <span class="math notranslate nohighlight">\( X_1, \dots, X_n \stackrel{idd}{\sim} \exp(\lambda)\)</span>, de paramètre inconnu <span class="math notranslate nohighlight">\( \lambda \geq 0\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*} f(x; \lambda) = \lambda \exp(-\lambda x), \hspace{1cm} x\geq 0.
\end{gather*}\]</div>
<p>Calculer <span class="math notranslate nohighlight">\( \hat{\lambda}_{ML}\)</span>.</p>
<div class="dropdown admonition">
<p class="admonition-title">Solution</p>
<p>Dans cet exemple, la log-vraisemblance est donnée par</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*} \ell(\lambda) = \sum_{i=1}^n \bigg( \log(\lambda) - \lambda X_i \bigg) = n \log(\lambda) - n \lambda \overline{X}.\end{gather*}\]</div>
<p>Calculons les dérivées:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
        \ell'(\lambda)  &amp;=\frac{n}{\lambda} - n \overline{X} \\
        \ell''(\lambda) &amp;= - \frac{n}{\lambda^2} &lt; 0,
    \end{align*}\]</div>
<p>et la fonction de log-vraisemblance est donc concave et tout point stationnaire est un maximum global.
Ce maximum global est:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*} \frac{n}{\hat{\lambda}_{ML}} = n \overline{X} \Leftrightarrow \hat{\lambda}_{ML}     = \frac{1}{\overline{X}} .\end{gather*}\]</div>
<p>De plus, <span class="math notranslate nohighlight">\( J\left(\hat{\lambda}_{ML}\right) \)</span> est donnée par</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*} J\left(\hat{\lambda}_{ML}\right) = - \left( -\frac{n}{\hat{\lambda}_{ML}^2}\right) = n \overline{X}^2.\end{gather*}\]</div>
<p>On en déduit la variance asymptotique:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}\var(\lambda_{ML}) = \frac{1}{n}\frac{1}{\overline{X}_n^2}. \end{equation*}\]</div>
</div>
</section>
</div><div class="proof theorem admonition" id="dist_asymp">
<p class="admonition-title"><span class="caption-number">Théorème 2.1 </span> (Distribution asymptotique de <span class="math notranslate nohighlight">\(\hat{\theta}_{ML}\)</span>)</p>
<section class="theorem-content" id="proof-content">
<p>Sous des hypothèses de régularité, l’estimateur <span class="math notranslate nohighlight">\(\hat{\theta}_{ML} \)</span> est asymptotiquement Gaussien, centré sur <span class="math notranslate nohighlight">\( \theta\)</span> et sa variance est donnée par moins l’inverse de la seconde dérivée de <span class="math notranslate nohighlight">\( \ell(\theta) \)</span>, évaluée en <span class="math notranslate nohighlight">\(\theta = \hat{\theta}_{ML} \)</span>. Autrement dit, on a</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} \hat{\theta}_{ML} \stackrel{\cdot}{\sim} \mathcal{N} \left(\theta, \frac{1}{J(\hat{\theta}_{ML})} \right), \quad J(\theta) = - \frac{\partial^2}{\partial \theta^2} \left[  \ell(\theta) \right]. 
\end{equation*}\]</div>
<p><span class="math notranslate nohighlight">\(J(\theta)\)</span> est appelé l’<em>information observée</em>.</p>
</section>
</div><figure class="align-default" id="log-like-estimators">
<a class="reference internal image-reference" href="../_images/log_like_estimators.svg"><img alt="../_images/log_like_estimators.svg" src="../_images/log_like_estimators.svg" width="95%" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.3 </span><span class="caption-text">: Fonctions de vraisemblances pour <span class="math notranslate nohighlight">\(N = 50\)</span> échantillons de taille <span class="math notranslate nohighlight">\(n =40 \)</span> chacun. Les points en rouge sont les maxima et le segment vertical représente la vraie valeur <span class="math notranslate nohighlight">\(\theta = 0\)</span>. En bleu, on voit la distribution normale dont les moments correspondent aux maxima.</span><a class="headerlink" href="#log-like-estimators" title="Lien permanent vers cette image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="qqplot-log-like-estimators">
<a class="reference internal image-reference" href="../_images/QQplot_log_like_estimators.svg"><img alt="../_images/QQplot_log_like_estimators.svg" src="../_images/QQplot_log_like_estimators.svg" width="95%" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.4 </span><span class="caption-text">: QQ-plot des estimateurs de maximum de vraisemblance correspondants.</span><a class="headerlink" href="#qqplot-log-like-estimators" title="Lien permanent vers cette image">#</a></p>
</figcaption>
</figure>
<p>Le choix d’un bon estimateur est un choix en général complexe.</p>
<p>Dans le cadre du cours, nous allons voir des situations courantes et décrire de bons estimateurs pour ces situations.</p>
<p><strong>Utilisez les estimateurs du cours dans ces situations courantes</strong></p>
<p>Connaître les méthodes des moindres carrés et du maximum de vraisemblance vous permet de déterminer des estimateurs si vous êtes confrontés à des <strong>nouvelles situations non-couvertes dans le cours</strong>.</p>
<p>Dans bon nombre de situations, on peut utiliser une méthode d’estimation « clés en main ». Cependant, ces solutions sont parfois plus ardues à utiliser qu’on ne le pense. Plusieurs aspects peuvent être délicats:</p>
<ul class="simple">
<li><p><strong>identifiabilité :</strong> parfois, plusieurs modèles très différents peuvent représenter de manière également performante des données. S’il y en a un, lequel est le bon?</p></li>
<li><p><strong>optimisation :</strong> la plupart des méthodes (dont le maximum de vraisemblance et les moindres carrés) reposent sur une procédure d’optimisation. Dans ce cas, des questions d’existence, d’unicité et de convergence des algorithmes d’optimisation entrent en ligne de compte.</p></li>
</ul>
<p>En résumé, tout est plus facile lorsque l’on cherche à optimiser une fonction convexe (ou concave): l’optimum existe, il est unique et des algorithmes efficients existent pour approcher la solution. Si ce n’est pas le cas, il faut prendre garde!</p>
<p>En particulier, il faut faire attention aux points suivants:</p>
<ul class="simple">
<li><p>vérifier les questions de convergences de l’algorithme d’optimisation. En particulier, étudier la sensibilité de l’estimation à la valeur de départ de l’algorithme. En général, il est judicieux d’utiliser une méthode facile pour avoir un point de départ proche de la valeur optimale, en utilisant par exemple la méthode des moments.</p></li>
<li><p>bien réfléchir au bien-fondé de l’estimation en utilisant les connaissances que l’on a du sujet. Se poser la question de la pertinence de la distribution résultante pour les <span class="math notranslate nohighlight">\(X_i\)</span>. Éventuellement on peut recourir à des méthodes différentes pour voir se elles donnent des résultats semblables.</p></li>
</ul>
<div class="proof example admonition" id="example-8">
<p class="admonition-title"><span class="caption-number">Exemple 2.7 </span> (Application de la distribution asymptotique de <span class="math notranslate nohighlight">\(\hat{\theta}_{ML}\)</span>)</p>
<section class="example-content" id="proof-content">
<p>Supposons que le temps (en secondes) nécessaire <span class="math notranslate nohighlight">\(X\)</span> pour résoudre un Rubik’s cube a pour fonction de densité</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*}
f_X(x;\theta)= \frac{1}{\delta \sqrt{2\pi}}  \frac{1}{x}   \exp \left \{ - \frac{1}{2\delta^2}\left(\ln x - \theta\right)^2 \right  \}, \quad
0&lt;x&lt;\infty, \quad \theta\in \mathbb{R},
\end{gather*}\]</div>
<p>où <span class="math notranslate nohighlight">\(\delta&gt; 0\)</span> est un paramètre fixé, considéré comme connu. On suppose qu’on observe un échantillon <span class="math notranslate nohighlight">\(X_1, \dots, X_n \stackrel{idd}{\sim} f_X\)</span>.
Déterminez l’estimateur du maximum de vraisemblance de <span class="math notranslate nohighlight">\(\theta\)</span>, <span class="math notranslate nohighlight">\(\hat \theta_{\text{ML}}\)</span> et en calculer la variance asymptotique de l’estimateur du maximum de vraisemblance.</p>
<div class="dropdown admonition">
<p class="admonition-title">Solution</p>
<p>On calcule tout d’abord la fonction de vraisemblance <span class="math notranslate nohighlight">\(L(\theta)\)</span> et on a</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
L(\theta)&amp; = \prod_{i = 1}^n f_{X_i}(x_i;\theta)= \frac{1}{\delta \sqrt{2\pi}}  \frac{1}{x_i}   \exp \left \{ - \frac{1}{2\delta^2}\left[\ln x_i - \theta\right]^2 \right\}\\
&amp; =(\delta \sqrt{2\pi})^{-n} \prod_{i = 1}^{n}\frac{1}{x_i}  \exp\left\{\sum_{i = 1}^n -\frac{1}{2\delta^2} \left[\ln(x_i) - \theta\right]^2 \right\},
\end{align*}\]</div>
<p>et la fonction de log-vraisemblance est donc</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*}\ell(\theta) = -n \ln(\delta \sqrt{2\pi}) -\sum_{i = 1}^n \ln(x_i) - \sum_{i = 1}^n \frac{1}{2\delta^2} \left[\ln(x_i) - \theta\right]^2. \end{gather*}\]</div>
<p>La dérivée de la fonction de log-vraisemblance est donnée par</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*}\ell'(\theta) = - \sum_{i = 1}^n \frac{1}{2\delta^2} \left[\ln(x_i) - \theta\right] \cdot 2 \cdot (-1) =  \sum_{i = 1}^n \frac{1}{\delta^2} \left[\ln(x_i) - \theta\right].\end{gather*}\]</div>
<p>L’estimateur <span class="math notranslate nohighlight">\(\hat \theta_{\text{ML}}\)</span> est un point stationnaire et satisfait</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp; \ell'(\hat \theta_{\text{ML}}) = 0 \Leftrightarrow \sum_{i = 1}^n \frac{1}{\delta^2} \left[\ln(x_i) - \hat \theta_{\text{ML}}\right] = 0 \Leftrightarrow  \frac{1}{\delta^2} \sum_{i = 1}^n \ln(x_i)  =  \frac{1}{\delta^2} n \hat \theta_{\text{ML}}\\
&amp; \Leftrightarrow \frac{1}{n}\sum_{i = 1}^n \ln(x_i) = \hat \theta_{\text{ML}}.
\end{align*}\]</div>
<p>Finalement, la fonction de log-vraisemblance est concave car</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*}\ell''(\theta) = \frac{-n}{\delta^2} &lt; 0. \end{gather*}\]</div>
<p>Ainsi, <span class="math notranslate nohighlight">\(\hat \theta_{\text{ML}}\)</span> est un maximum global et donc l’estimateur du maximum de vraisemblance.</p>
<p>En utilisant le théorème sur la distribution asymptotique de l’estimateur du maximum de vraisemblance, on a</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{gather*} \var(\hat \theta_{\text{ML}}) = [-\ell''(\hat \theta_{\text{ML}}) ]^{-1} = \frac{\delta^2}{n}.\end{gather*}\]</div>
</div>
</section>
</div></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "MatthWilhelm/ProbaStat",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Statistique_inférentielle"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="introduction.html" title="précédent page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">précédent</p>
            <p class="prev-next-title"><span class="section-number">1. </span>Introduction</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="proprietes_estimateur.html" title="suivant page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">suivant</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Propriétés d’un estimateur</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Matthieu Wilhelm et Kieran Vaudaux<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>